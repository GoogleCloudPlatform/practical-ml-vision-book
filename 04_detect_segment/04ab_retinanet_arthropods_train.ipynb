{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<table class=\"tfo-notebook-buttons\" align=\"left\"><td><a target=\"_blank\" href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?name=Object+Detection+with+RetinaNet+on+Arthropods+dataset+%2F+training&download_url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fpractical-ml-vision-book%2Fraw%2Fmaster%2F04_detect_segment%2F04ab_retinanet_arthropods_train.ipynb\"><img src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\"/>Run in AI Platform Notebook</a></td><td><a target=\"_blank\" href=\"https://colab.research.google.com/github/GoogleCloudPlatform/practical-ml-vision-book/blob/master/04_detect_segment/04ab_retinanet_arthropods_train.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a></td><td><a target=\"_blank\" href=\"https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/04_detect_segment/04ab_retinanet_arthropods_train.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td><td><a href=\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/04_detect_segment/04ab_retinanet_arthropods_train.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a></td></table><br/><br/><h1>Object Detection with RetinaNet on Arthropods dataset / training</h1>This notebook is set up to run on TPU or GPU. It has been executed on a TPUv3 but it works fine on TPUv2 (Colaboratory). Training on TPU requires a private writable GCS bucket. See the GCS bucket section below. This example uses the RetinaNet implementation from Tensorflow model Garden."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "from IPython.display import Markdown as md\n",
    "_nb_loc = \"04_detect_segment/04ab_retinanet_arthropods_train.ipynb\" # change to reflect your notebook\n",
    "_nb_title = \"Object Detection with RetinaNet on Arthropods dataset / training\" # change to reflect your notebook\n",
    "_nb_message = \"This notebook is set up to run on TPU or GPU. It has been executed on a TPUv3 but it works fine on TPUv2 (Colaboratory). Training on TPU requires a private writable GCS bucket. See the GCS bucket section below. This example uses the RetinaNet implementation from Tensorflow model Garden.\" # change to reflect your notebook\n",
    "_icons=[\"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/logo-cloud.png\", \"https://www.tensorflow.org/images/colab_logo_32px.png\", \"https://www.tensorflow.org/images/GitHub-Mark-32px.png\", \"https://www.tensorflow.org/images/download_logo_32px.png\"]\n",
    "_links=[\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?\" + urllib.parse.urlencode({\"name\": _nb_title, \"download_url\": \"https://github.com/GoogleCloudPlatform/practical-ml-vision-book/raw/master/\"+_nb_loc}), \"https://colab.research.google.com/github/GoogleCloudPlatform/practical-ml-vision-book/blob/master/{0}\".format(_nb_loc), \"https://github.com/GoogleCloudPlatform/practical-ml-vision-book/blob/master/{0}\".format(_nb_loc), \"https://raw.githubusercontent.com/GoogleCloudPlatform/practical-ml-vision-book/master/{0}\".format(_nb_loc)]\n",
    "md(\"\"\"<table class=\"tfo-notebook-buttons\" align=\"left\"><td><a target=\"_blank\" href=\"{0}\"><img src=\"{4}\"/>Run in AI Platform Notebook</a></td><td><a target=\"_blank\" href=\"{1}\"><img src=\"{5}\" />Run in Google Colab</a></td><td><a target=\"_blank\" href=\"{2}\"><img src=\"{6}\" />View source on GitHub</a></td><td><a href=\"{3}\"><img src=\"{7}\" />Download notebook</a></td></table><br/><br/><h1>{8}</h1>{9}\"\"\".format(_links[0], _links[1], _links[2], _links[3], _icons[0], _icons[1], _icons[2], _icons[3], _nb_title, _nb_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet tf-models-official==2.5\n",
    "# please restart the kernel after installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.7.0\n"
     ]
    }
   ],
   "source": [
    "import time, re, os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "print(\"Tensorflow version\", tf.__version__)\n",
    "\n",
    "# Tensorflow Model Garden imports\n",
    "import official as model_garden\n",
    "from official.vision.beta.configs import retinanet as retinanet_cfg\n",
    "from official.vision.beta.configs import backbones as backbones_cfg\n",
    "from official.vision.beta.serving import export_saved_model_lib\n",
    "from official.core import train_lib\n",
    "\n",
    "# TODO\n",
    "# load the backbone checkpoint from the official loacation as soon as it is published\n",
    "# save the model configuration to the saved_odel folder as per best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCS bucket\n",
    "This bucket will receive:\n",
    " - Tensorboard summaries that allow you to follow the training\n",
    " - checkpoints\n",
    " - the saved model after training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your own GCS bucket here. GCS is required if training on TPU.\n",
    "# On GPU, a local folder will work.\n",
    "MODEL_ARTIFACT_BUCKET = 'gs://ml1-demo-martin/arthropod_jobs/'\n",
    "MODEL_DIR = MODEL_ARTIFACT_BUCKET + str(int(time.time()))\n",
    "\n",
    "# If you are running on Colaboratory, you must authenticate\n",
    "# for Colab to have write access to the bucket.\n",
    "\n",
    "IS_COLAB_BACKEND = 'COLAB_GPU' in os.environ  # this is always set on Colab, the value is 0 or 1 depending on GPU presence\n",
    "if IS_COLAB_BACKEND:\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPU / GPU detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-24 23:20:33.980914: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2021-11-24 23:20:34.013046: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.28.166.186:8470}\n",
      "2021-11-24 23:20:34.013088: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:38932}\n",
      "2021-11-24 23:20:34.031838: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job worker -> {0 -> 10.28.166.186:8470}\n",
      "2021-11-24 23:20:34.031883: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:272] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:38932}\n",
      "2021-11-24 23:20:34.035122: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:427] Started server with target: grpc://localhost:38932\n",
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: martin-tpuv3-8-tf27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: martin-tpuv3-8-tf27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.get_strategy()\n",
    "\n",
    "try: # detect TPUs\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except ValueError: # detect GPUs or multi-GPU machines\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model dir: gs://ml1-demo-martin/arthropod_jobs/1637796032\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_PATH_PATTERN = 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.train.tfrec'\n",
    "VALID_DATA_PATH_PATTERN = 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.test.tfrec'\n",
    "SPINET_MOBILE_CHECKPOINT = 'gs://practical-ml-vision-book/arthropod_detection_tfr/spinenet_mobile_checkpoint/'\n",
    "\n",
    "BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n",
    "\n",
    "EPOCHS = 80\n",
    "\n",
    "RAW_CLASSES = ['Lepidoptera', 'Hymenoptera', 'Hemiptera', 'Odonata', 'Diptera', 'Araneae', 'Coleoptera',\n",
    "               '_truncated', '_blurred', '_occluded', ]\n",
    "CLASSES = [klass for klass in RAW_CLASSES if klass not in ['_truncated', '_blurred', '_occluded']]\n",
    "\n",
    "# Lepidoptera = butterfies and moths\n",
    "# Hymenoptera = wasps, bees and ants\n",
    "# Hemiptera = true bugs (cicadas, aphids, shield bugs, ...)\n",
    "# Odonata = dragonflies\n",
    "# Diptera = fies\n",
    "# Araneae = spiders\n",
    "# Coleoptera = beetles\n",
    "\n",
    "# NOT IN DATASET\n",
    "# Orthoptera = grasshoppers\n",
    "\n",
    "print(\"Model dir:\", MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data files\n",
    "The dataset is already prepared in TFRecord format.<br/>\n",
    "The script that prepared the data is in \"04aa_retinanet_arthropods_dataprep.ipynb\"<br/>\n",
    "To parse the TFRecord files by hand and visulaize their contents, see code in \"04ac_retinanet_arthropods_predict.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:\n",
      "    24 TFRecord files.\n",
      "    11544 images\n",
      "    Steps per epoch: 45\n",
      "\n",
      "Validation dataset:\n",
      "    8 TFRecord files.\n",
      "    3832 images\n",
      "    Validation steps: 14\n",
      "\n",
      "Global batch size: 256\n"
     ]
    }
   ],
   "source": [
    "def count_data_items(filenames):\n",
    "    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return int(np.sum(n))\n",
    "\n",
    "TRAIN_FILENAMES = tf.io.gfile.glob(TRAIN_DATA_PATH_PATTERN)\n",
    "NB_TRAIN_IMAGES = count_data_items(TRAIN_FILENAMES)\n",
    "STEPS_PER_EPOCH = NB_TRAIN_IMAGES // BATCH_SIZE\n",
    "\n",
    "VALID_FILENAMES = tf.io.gfile.glob(VALID_DATA_PATH_PATTERN)\n",
    "NB_VALID_IMAGES = count_data_items(VALID_FILENAMES)\n",
    "VALID_STEPS = NB_VALID_IMAGES // BATCH_SIZE\n",
    "\n",
    "print(\"Training dataset:\")\n",
    "print(f\"    {len(TRAIN_FILENAMES)} TFRecord files.\")\n",
    "print(f\"    {NB_TRAIN_IMAGES} images\")\n",
    "print(\"    Steps per epoch:\", STEPS_PER_EPOCH)\n",
    "print()\n",
    "print(\"Validation dataset:\")\n",
    "print(f\"    {len(VALID_FILENAMES)} TFRecord files.\")\n",
    "print(f\"    {NB_VALID_IMAGES} images\")\n",
    "print(\"    Validation steps:\", VALID_STEPS)\n",
    "print()\n",
    "print(\"Global batch size:\", BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'runtime': {'all_reduce_alg': None,\n",
      "             'batchnorm_spatial_persistent': False,\n",
      "             'dataset_num_private_threads': None,\n",
      "             'default_shard_dim': -1,\n",
      "             'distribution_strategy': 'mirrored',\n",
      "             'enable_xla': False,\n",
      "             'gpu_thread_mode': None,\n",
      "             'loss_scale': None,\n",
      "             'mixed_precision_dtype': None,\n",
      "             'num_cores_per_replica': 1,\n",
      "             'num_gpus': 0,\n",
      "             'num_packs': 1,\n",
      "             'per_gpu_thread_count': 0,\n",
      "             'run_eagerly': False,\n",
      "             'task_index': -1,\n",
      "             'tpu': None,\n",
      "             'tpu_enable_xla_dynamic_padder': None,\n",
      "             'worker_hosts': None},\n",
      " 'task': {'annotation_file': None,\n",
      "          'init_checkpoint': 'gs://practical-ml-vision-book/arthropod_detection_tfr/spinenet_mobile_checkpoint/',\n",
      "          'init_checkpoint_modules': 'backbone',\n",
      "          'losses': {'box_loss_weight': 50,\n",
      "                     'focal_loss_alpha': 0.25,\n",
      "                     'focal_loss_gamma': 1.5,\n",
      "                     'huber_loss_delta': 0.1,\n",
      "                     'l2_weight_decay': 0.0},\n",
      "          'model': {'anchor': {'anchor_size': 4.0,\n",
      "                               'aspect_ratios': [0.5, 1.0, 2.0],\n",
      "                               'num_scales': 3},\n",
      "                    'backbone': {'spinenet_mobile': {'expand_ratio': 6,\n",
      "                                                     'model_id': '49',\n",
      "                                                     'se_ratio': 0.2,\n",
      "                                                     'stochastic_depth_drop_rate': 0.0},\n",
      "                                 'type': 'spinenet_mobile'},\n",
      "                    'decoder': {'fpn': {'num_filters': 256,\n",
      "                                        'use_separable_conv': False},\n",
      "                                'type': 'fpn'},\n",
      "                    'detection_generator': {'max_num_detections': 100,\n",
      "                                            'nms_iou_threshold': 0.5,\n",
      "                                            'pre_nms_score_threshold': 0.05,\n",
      "                                            'pre_nms_top_k': 5000,\n",
      "                                            'use_batched_nms': False},\n",
      "                    'head': {'num_convs': 4,\n",
      "                             'num_filters': 256,\n",
      "                             'use_separable_conv': False},\n",
      "                    'input_size': [384, 384, 3],\n",
      "                    'max_level': 7,\n",
      "                    'min_level': 3,\n",
      "                    'norm_activation': {'activation': 'relu',\n",
      "                                        'norm_epsilon': 0.001,\n",
      "                                        'norm_momentum': 0.99,\n",
      "                                        'use_sync_bn': True},\n",
      "                    'num_classes': 8},\n",
      "          'per_category_metrics': False,\n",
      "          'train_data': {'block_length': 1,\n",
      "                         'cache': False,\n",
      "                         'cycle_length': None,\n",
      "                         'decoder': {'simple_decoder': {'regenerate_source_id': False},\n",
      "                                     'type': 'simple_decoder'},\n",
      "                         'deterministic': None,\n",
      "                         'drop_remainder': True,\n",
      "                         'dtype': 'bfloat16',\n",
      "                         'enable_tf_data_service': False,\n",
      "                         'file_type': 'tfrecord',\n",
      "                         'global_batch_size': 256,\n",
      "                         'input_path': 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.train.tfrec',\n",
      "                         'is_training': True,\n",
      "                         'parser': {'aug_rand_hflip': True,\n",
      "                                    'aug_scale_max': 2.0,\n",
      "                                    'aug_scale_min': 0.7,\n",
      "                                    'match_threshold': 0.5,\n",
      "                                    'max_num_instances': 100,\n",
      "                                    'num_channels': 3,\n",
      "                                    'skip_crowd_during_training': True,\n",
      "                                    'unmatched_threshold': 0.5},\n",
      "                         'seed': None,\n",
      "                         'sharding': True,\n",
      "                         'shuffle_buffer_size': 10000,\n",
      "                         'tf_data_service_address': None,\n",
      "                         'tf_data_service_job_name': None,\n",
      "                         'tfds_as_supervised': False,\n",
      "                         'tfds_data_dir': '',\n",
      "                         'tfds_name': '',\n",
      "                         'tfds_skip_decoding_feature': '',\n",
      "                         'tfds_split': ''},\n",
      "          'validation_data': {'block_length': 1,\n",
      "                              'cache': False,\n",
      "                              'cycle_length': None,\n",
      "                              'decoder': {'simple_decoder': {'regenerate_source_id': False},\n",
      "                                          'type': 'simple_decoder'},\n",
      "                              'deterministic': None,\n",
      "                              'drop_remainder': True,\n",
      "                              'dtype': 'bfloat16',\n",
      "                              'enable_tf_data_service': False,\n",
      "                              'file_type': 'tfrecord',\n",
      "                              'global_batch_size': 256,\n",
      "                              'input_path': 'gs://practical-ml-vision-book/arthropod_detection_tfr/size_w1024px/*.test.tfrec',\n",
      "                              'is_training': False,\n",
      "                              'parser': {'aug_rand_hflip': False,\n",
      "                                         'aug_scale_max': 1.0,\n",
      "                                         'aug_scale_min': 1.0,\n",
      "                                         'match_threshold': 0.5,\n",
      "                                         'max_num_instances': 100,\n",
      "                                         'num_channels': 3,\n",
      "                                         'skip_crowd_during_training': True,\n",
      "                                         'unmatched_threshold': 0.5},\n",
      "                              'seed': None,\n",
      "                              'sharding': True,\n",
      "                              'shuffle_buffer_size': 10000,\n",
      "                              'tf_data_service_address': None,\n",
      "                              'tf_data_service_job_name': None,\n",
      "                              'tfds_as_supervised': False,\n",
      "                              'tfds_data_dir': '',\n",
      "                              'tfds_name': '',\n",
      "                              'tfds_skip_decoding_feature': '',\n",
      "                              'tfds_split': ''}},\n",
      " 'trainer': {'allow_tpu_summary': False,\n",
      "             'best_checkpoint_eval_metric': '',\n",
      "             'best_checkpoint_export_subdir': '',\n",
      "             'best_checkpoint_metric_comp': 'higher',\n",
      "             'checkpoint_interval': 360,\n",
      "             'continuous_eval_timeout': 3600,\n",
      "             'eval_tf_function': True,\n",
      "             'eval_tf_while_loop': False,\n",
      "             'loss_upper_bound': 1000000.0,\n",
      "             'max_to_keep': 5,\n",
      "             'optimizer_config': {'ema': None,\n",
      "                                  'learning_rate': {'stepwise': {'boundaries': [675,\n",
      "                                                                                1350,\n",
      "                                                                                2025,\n",
      "                                                                                2700,\n",
      "                                                                                3375],\n",
      "                                                                 'name': 'PiecewiseConstantDecay',\n",
      "                                                                 'values': [0.016,\n",
      "                                                                            0.008,\n",
      "                                                                            0.004,\n",
      "                                                                            0.002,\n",
      "                                                                            0.001,\n",
      "                                                                            0.0005]},\n",
      "                                                    'type': 'stepwise'},\n",
      "                                  'optimizer': {'sgd': {'clipnorm': None,\n",
      "                                                        'clipvalue': None,\n",
      "                                                        'decay': 0.0,\n",
      "                                                        'global_clipnorm': None,\n",
      "                                                        'momentum': 0.9,\n",
      "                                                        'name': 'SGD',\n",
      "                                                        'nesterov': False},\n",
      "                                                'type': 'sgd'},\n",
      "                                  'warmup': {'type': None}},\n",
      "             'recovery_begin_steps': 0,\n",
      "             'recovery_max_trials': 0,\n",
      "             'steps_per_loop': 45,\n",
      "             'summary_interval': 45,\n",
      "             'train_steps': 3600,\n",
      "             'train_tf_function': True,\n",
      "             'train_tf_while_loop': True,\n",
      "             'validation_interval': 360,\n",
      "             'validation_steps': 14}}\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = [384, 384]\n",
    "\n",
    "# default parameters can be overriden in two ways:\n",
    "# 1) params.override({'task': {'model': {'backbone': backbone_cfg.as_dict()}}})\n",
    "# 2) params.task.model.backbone = backbone_cfg\n",
    "# params.override checks that the dictionary keys exist\n",
    "# the second options will silently add new keys\n",
    "\n",
    "params = model_garden.core.exp_factory.get_exp_config('retinanet')\n",
    "\n",
    "params.task.model.num_classes = len(CLASSES)+1 # class 0 is reserved for backgrounds\n",
    "params.task.model.input_size = [*IMAGE_SIZE, 3] # this automatically configures the input reader to random crop training images\n",
    "params.task.init_checkpoint = SPINET_MOBILE_CHECKPOINT\n",
    "params.task.init_checkpoint_modules = 'backbone'\n",
    "params.task.model.backbone = backbones_cfg.Backbone(type='spinenet_mobile', spinenet_mobile=backbones_cfg.SpineNetMobile())\n",
    "\n",
    "train_data_cfg=retinanet_cfg.DataConfig(\n",
    "    input_path=TRAIN_DATA_PATH_PATTERN,\n",
    "    is_training=True,\n",
    "    global_batch_size=BATCH_SIZE,\n",
    "    parser=retinanet_cfg.Parser(aug_rand_hflip=True, aug_scale_min=0.7, aug_scale_max=2.0))\n",
    "\n",
    "valid_data_cfg=retinanet_cfg.DataConfig(\n",
    "    input_path=VALID_DATA_PATH_PATTERN,\n",
    "    is_training=False,\n",
    "    global_batch_size=BATCH_SIZE)\n",
    "\n",
    "params.override({'task': {'train_data': train_data_cfg.as_dict(), 'validation_data': valid_data_cfg.as_dict()}})\n",
    "\n",
    "trainer_cfg=model_garden.core.config_definitions.TrainerConfig(\n",
    "    train_steps=EPOCHS * STEPS_PER_EPOCH,\n",
    "    validation_steps=VALID_STEPS,\n",
    "    validation_interval=8*STEPS_PER_EPOCH,\n",
    "    steps_per_loop=STEPS_PER_EPOCH,\n",
    "    summary_interval=STEPS_PER_EPOCH,\n",
    "    checkpoint_interval=8*STEPS_PER_EPOCH)\n",
    "\n",
    "optim_cfg = model_garden.modeling.optimization.OptimizationConfig({\n",
    "    'optimizer': {\n",
    "                  'type': 'sgd',\n",
    "                  'sgd': {'momentum': 0.9}},\n",
    "    'learning_rate': {'type': 'stepwise',\n",
    "                      'stepwise': {'boundaries': [15 * STEPS_PER_EPOCH,\n",
    "                                                  30 * STEPS_PER_EPOCH,\n",
    "                                                  45 * STEPS_PER_EPOCH,\n",
    "                                                  60 * STEPS_PER_EPOCH,\n",
    "                                                  75 * STEPS_PER_EPOCH],\n",
    "                                   'values': [0.016, #0.01,\n",
    "                                              0.008, #0.005,\n",
    "                                              0.004, #0.0025,\n",
    "                                              0.002, #0.001,\n",
    "                                              0.001, #0.0005,\n",
    "                                              0.0005]} #0.00025]}\n",
    "                     },\n",
    "    #'warmup': {'type': 'linear','linear': {'warmup_steps': 5*STEPS_PER_EPOCH, 'warmup_learning_rate': 0.00001}}\n",
    "})\n",
    "\n",
    "trainer_cfg.override({'optimizer_config': optim_cfg})\n",
    "params.override({'trainer': trainer_cfg})\n",
    "\n",
    "pp.pprint(params.as_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = model_garden.core.task_factory.get_task(params.task, logging_dir=MODEL_DIR)\n",
    "\n",
    "# this works too:\n",
    "#task = official.vision.beta.tasks.retinanet.RetinaNetTask(params.task)\n",
    "\n",
    "# this returns a RetinaNetModel\n",
    "#task.build_model()\n",
    "# note: none of the expected model functionalities work: model.fit(), model.predict(), model.save()\n",
    "\n",
    "# this returns the training dataset\n",
    "#train_dataset = task.build_inputs(train_data_cfg)\n",
    "# note: the dataset already includes FPN level and anchor pairing and is therefore not very readable\n",
    "\n",
    "# this returns the validation dataset\n",
    "#valid_dataset = task.build_inputs(valid_data_cfg)\n",
    "# note: the dataset already includes FPN level and anchor pairing and is therefore not very readable\n",
    "\n",
    "# this code allows you to see if the TFRecord fields are read correctly\n",
    "#ds = tf.data.TFRecordDataset(tf.io.gfile.glob(TRAIN_DATA_PATH_PATTERN))\n",
    "#dec = official.vision.beta.dataloaders.tf_example_decoder.TfExampleDecoder()\n",
    "#ds = ds.map(dec.decode)\n",
    "\n",
    "# training and validatoin data parsing happens in:\n",
    "# official.vision.beta.dataloaders.retinanet_input.Parser._parse_train_data\n",
    "# official.vision.beta.dataloaders.retinanet_input.Parser._parse_eval_data\n",
    "# official.vision.beta.dataloaders.Parser.parse() # dispatches between _parse_train_data and _parse_eval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "Training takes approximately 30min on a TPUv3-8, 40min on a TPUv2-8 on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ml1-demo-martin/arthropod_jobs/1637796032\n",
      "restoring or initializing model...\n",
      "initialized model.\n",
      "train | step:      0 | training until step 360...\n",
      "train | step:     45 | steps/sec:    0.1 | output: \n",
      "    {'box_loss': 0.009556498,\n",
      "     'cls_loss': 0.90980715,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 1.3876321,\n",
      "     'total_loss': 1.3876321,\n",
      "     'training_loss': 1.3876321}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/1637796032/ckpt-45.\n",
      "train | step:     90 | steps/sec:    1.8 | output: \n",
      "    {'box_loss': 0.005307368,\n",
      "     'cls_loss': 0.63911986,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.90448844,\n",
      "     'total_loss': 0.90448844,\n",
      "     'training_loss': 0.90448844}\n",
      "train | step:    135 | steps/sec:    2.8 | output: \n",
      "    {'box_loss': 0.004365041,\n",
      "     'cls_loss': 0.5586993,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.77695125,\n",
      "     'total_loss': 0.77695125,\n",
      "     'training_loss': 0.77695125}\n",
      "train | step:    180 | steps/sec:    2.6 | output: \n",
      "    {'box_loss': 0.0039574234,\n",
      "     'cls_loss': 0.5234108,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.721282,\n",
      "     'total_loss': 0.721282,\n",
      "     'training_loss': 0.721282}\n",
      "train | step:    225 | steps/sec:    2.7 | output: \n",
      "    {'box_loss': 0.003681702,\n",
      "     'cls_loss': 0.49403378,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.67811877,\n",
      "     'total_loss': 0.67811877,\n",
      "     'training_loss': 0.67811877}\n",
      "train | step:    270 | steps/sec:    2.6 | output: \n",
      "    {'box_loss': 0.003496634,\n",
      "     'cls_loss': 0.47317967,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.6480113,\n",
      "     'total_loss': 0.6480113,\n",
      "     'training_loss': 0.6480113}\n",
      "train | step:    315 | steps/sec:    2.6 | output: \n",
      "    {'box_loss': 0.0032965434,\n",
      "     'cls_loss': 0.44756672,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.612394,\n",
      "     'total_loss': 0.612394,\n",
      "     'training_loss': 0.612394}\n",
      "train | step:    360 | steps/sec:    2.5 | output: \n",
      "    {'box_loss': 0.0032442657,\n",
      "     'cls_loss': 0.43660975,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.5988229,\n",
      "     'total_loss': 0.5988229,\n",
      "     'training_loss': 0.5988229}\n",
      " eval | step:    360 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=16.45s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.30s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.268\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.430\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.289\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.015\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.288\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.487\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.559\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.566\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.071\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.610\n",
      " eval | step:    360 | eval time:   81.3 sec | output: \n",
      "    {'AP': 0.26832336,\n",
      "     'AP50': 0.4300787,\n",
      "     'AP75': 0.28862187,\n",
      "     'APl': 0.28750116,\n",
      "     'APm': 0.015297678,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.61047405,\n",
      "     'ARm': 0.0706937,\n",
      "     'ARmax1': 0.48737863,\n",
      "     'ARmax10': 0.5594851,\n",
      "     'ARmax100': 0.5661079,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0037560686,\n",
      "     'cls_loss': 0.5365863,\n",
      "     'model_loss': 0.72438973,\n",
      "     'total_loss': 0.72438973,\n",
      "     'validation_loss': 0.72438973}\n",
      "train | step:    360 | training until step 720...\n",
      "train | step:    405 | steps/sec:    0.4 | output: \n",
      "    {'box_loss': 0.003196666,\n",
      "     'cls_loss': 0.42111766,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.58095086,\n",
      "     'total_loss': 0.58095086,\n",
      "     'training_loss': 0.58095086}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/1637796032/ckpt-405.\n",
      "train | step:    450 | steps/sec:    2.0 | output: \n",
      "    {'box_loss': 0.0029801917,\n",
      "     'cls_loss': 0.40611297,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.5551226,\n",
      "     'total_loss': 0.5551226,\n",
      "     'training_loss': 0.5551226}\n",
      "train | step:    495 | steps/sec:    3.0 | output: \n",
      "    {'box_loss': 0.0029069097,\n",
      "     'cls_loss': 0.39142147,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.5367669,\n",
      "     'total_loss': 0.5367669,\n",
      "     'training_loss': 0.5367669}\n",
      "train | step:    540 | steps/sec:    2.7 | output: \n",
      "    {'box_loss': 0.0028927405,\n",
      "     'cls_loss': 0.38078177,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.5254188,\n",
      "     'total_loss': 0.5254188,\n",
      "     'training_loss': 0.5254188}\n",
      "train | step:    585 | steps/sec:    2.7 | output: \n",
      "    {'box_loss': 0.002818517,\n",
      "     'cls_loss': 0.36946267,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.5103886,\n",
      "     'total_loss': 0.5103886,\n",
      "     'training_loss': 0.5103886}\n",
      "train | step:    630 | steps/sec:    2.5 | output: \n",
      "    {'box_loss': 0.002743276,\n",
      "     'cls_loss': 0.363299,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.5004627,\n",
      "     'total_loss': 0.5004627,\n",
      "     'training_loss': 0.5004627}\n",
      "train | step:    675 | steps/sec:    2.6 | output: \n",
      "    {'box_loss': 0.0027198468,\n",
      "     'cls_loss': 0.35987926,\n",
      "     'learning_rate': 0.016,\n",
      "     'model_loss': 0.4958716,\n",
      "     'total_loss': 0.4958716,\n",
      "     'training_loss': 0.4958716}\n",
      "train | step:    720 | steps/sec:    2.7 | output: \n",
      "    {'box_loss': 0.0026025185,\n",
      "     'cls_loss': 0.3406233,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4707493,\n",
      "     'total_loss': 0.4707493,\n",
      "     'training_loss': 0.4707493}\n",
      " eval | step:    720 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=18.73s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.50s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.394\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.585\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.431\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.032\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.423\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.536\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.611\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.620\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.120\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.665\n",
      " eval | step:    720 | eval time:   54.0 sec | output: \n",
      "    {'AP': 0.39413595,\n",
      "     'AP50': 0.5847048,\n",
      "     'AP75': 0.43115857,\n",
      "     'APl': 0.42292807,\n",
      "     'APm': 0.03156409,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.66479313,\n",
      "     'ARm': 0.120165594,\n",
      "     'ARmax1': 0.53558916,\n",
      "     'ARmax10': 0.611178,\n",
      "     'ARmax100': 0.6198528,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0031289435,\n",
      "     'cls_loss': 0.42072892,\n",
      "     'model_loss': 0.57717615,\n",
      "     'total_loss': 0.57717615,\n",
      "     'validation_loss': 0.57717615}\n",
      "train | step:    720 | training until step 1080...\n",
      "train | step:    765 | steps/sec:    0.6 | output: \n",
      "    {'box_loss': 0.0025358277,\n",
      "     'cls_loss': 0.33305147,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4598429,\n",
      "     'total_loss': 0.4598429,\n",
      "     'training_loss': 0.4598429}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/1637796032/ckpt-765.\n",
      "train | step:    810 | steps/sec:    2.0 | output: \n",
      "    {'box_loss': 0.0025076002,\n",
      "     'cls_loss': 0.3298932,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4552733,\n",
      "     'total_loss': 0.4552733,\n",
      "     'training_loss': 0.4552733}\n",
      "train | step:    855 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.0024614497,\n",
      "     'cls_loss': 0.31960323,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.44267583,\n",
      "     'total_loss': 0.44267583,\n",
      "     'training_loss': 0.44267583}\n",
      "train | step:    900 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.0024891277,\n",
      "     'cls_loss': 0.3195183,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.44397464,\n",
      "     'total_loss': 0.44397464,\n",
      "     'training_loss': 0.44397464}\n",
      "train | step:    945 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.0024845337,\n",
      "     'cls_loss': 0.31656307,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.44078976,\n",
      "     'total_loss': 0.44078976,\n",
      "     'training_loss': 0.44078976}\n",
      "train | step:    990 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.0024048975,\n",
      "     'cls_loss': 0.31188053,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.43212542,\n",
      "     'total_loss': 0.43212542,\n",
      "     'training_loss': 0.43212542}\n",
      "train | step:   1035 | steps/sec:    2.5 | output: \n",
      "    {'box_loss': 0.0024089003,\n",
      "     'cls_loss': 0.30855688,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.42900178,\n",
      "     'total_loss': 0.42900178,\n",
      "     'training_loss': 0.42900178}\n",
      "train | step:   1080 | steps/sec:    2.5 | output: \n",
      "    {'box_loss': 0.002367836,\n",
      "     'cls_loss': 0.30211505,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4205069,\n",
      "     'total_loss': 0.4205069,\n",
      "     'training_loss': 0.4205069}\n",
      " eval | step:   1080 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=30.20s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=7.51s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.440\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.635\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.486\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.473\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.551\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.629\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.636\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.130\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.681\n",
      " eval | step:   1080 | eval time:   71.8 sec | output: \n",
      "    {'AP': 0.4395279,\n",
      "     'AP50': 0.6351018,\n",
      "     'AP75': 0.48583257,\n",
      "     'APl': 0.47268134,\n",
      "     'APm': 0.029759092,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.6810471,\n",
      "     'ARm': 0.13044845,\n",
      "     'ARmax1': 0.5510549,\n",
      "     'ARmax10': 0.62891674,\n",
      "     'ARmax100': 0.63586366,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0029519496,\n",
      "     'cls_loss': 0.3816042,\n",
      "     'model_loss': 0.52920175,\n",
      "     'total_loss': 0.52920175,\n",
      "     'validation_loss': 0.52920175}\n",
      "train | step:   1080 | training until step 1440...\n",
      "train | step:   1125 | steps/sec:    0.5 | output: \n",
      "    {'box_loss': 0.0023988287,\n",
      "     'cls_loss': 0.30283782,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.4227792,\n",
      "     'total_loss': 0.4227792,\n",
      "     'training_loss': 0.4227792}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/1637796032/ckpt-1125.\n",
      "train | step:   1170 | steps/sec:    1.8 | output: \n",
      "    {'box_loss': 0.00241824,\n",
      "     'cls_loss': 0.3003762,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.42128822,\n",
      "     'total_loss': 0.42128822,\n",
      "     'training_loss': 0.42128822}\n",
      "train | step:   1215 | steps/sec:    3.4 | output: \n",
      "    {'box_loss': 0.0023461268,\n",
      "     'cls_loss': 0.29723275,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.41453904,\n",
      "     'total_loss': 0.41453904,\n",
      "     'training_loss': 0.41453904}\n",
      "train | step:   1260 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.0023003963,\n",
      "     'cls_loss': 0.29182583,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.40684563,\n",
      "     'total_loss': 0.40684563,\n",
      "     'training_loss': 0.40684563}\n",
      "train | step:   1305 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.00229992,\n",
      "     'cls_loss': 0.28828892,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.40328488,\n",
      "     'total_loss': 0.40328488,\n",
      "     'training_loss': 0.40328488}\n",
      "train | step:   1350 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.0022527974,\n",
      "     'cls_loss': 0.28270146,\n",
      "     'learning_rate': 0.008,\n",
      "     'model_loss': 0.39534134,\n",
      "     'total_loss': 0.39534134,\n",
      "     'training_loss': 0.39534134}\n",
      "train | step:   1395 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.0022971758,\n",
      "     'cls_loss': 0.2839757,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3988345,\n",
      "     'total_loss': 0.3988345,\n",
      "     'training_loss': 0.3988345}\n",
      "train | step:   1440 | steps/sec:    2.1 | output: \n",
      "    {'box_loss': 0.0022499117,\n",
      "     'cls_loss': 0.27730593,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3898015,\n",
      "     'total_loss': 0.3898015,\n",
      "     'training_loss': 0.3898015}\n",
      " eval | step:   1440 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=21.92s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=6.11s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.463\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.663\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.509\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.499\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.560\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.635\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.643\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.127\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.688\n",
      " eval | step:   1440 | eval time:   65.3 sec | output: \n",
      "    {'AP': 0.46322942,\n",
      "     'AP50': 0.66332775,\n",
      "     'AP75': 0.50923884,\n",
      "     'APl': 0.4986061,\n",
      "     'APm': 0.02960428,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.6884273,\n",
      "     'ARm': 0.12729502,\n",
      "     'ARmax1': 0.5600275,\n",
      "     'ARmax10': 0.6354349,\n",
      "     'ARmax100': 0.6425564,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0028825796,\n",
      "     'cls_loss': 0.3665981,\n",
      "     'model_loss': 0.51072705,\n",
      "     'total_loss': 0.51072705,\n",
      "     'validation_loss': 0.51072705}\n",
      "train | step:   1440 | training until step 1800...\n",
      "train | step:   1485 | steps/sec:    0.5 | output: \n",
      "    {'box_loss': 0.0022144045,\n",
      "     'cls_loss': 0.27389258,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.38461286,\n",
      "     'total_loss': 0.38461286,\n",
      "     'training_loss': 0.38461286}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/1637796032/ckpt-1485.\n",
      "train | step:   1530 | steps/sec:    1.9 | output: \n",
      "    {'box_loss': 0.0022185734,\n",
      "     'cls_loss': 0.27217346,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3831022,\n",
      "     'total_loss': 0.3831022,\n",
      "     'training_loss': 0.3831022}\n",
      "train | step:   1575 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.0022311488,\n",
      "     'cls_loss': 0.27272242,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.38427985,\n",
      "     'total_loss': 0.38427985,\n",
      "     'training_loss': 0.38427985}\n",
      "train | step:   1620 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.0021750403,\n",
      "     'cls_loss': 0.26941618,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3781682,\n",
      "     'total_loss': 0.3781682,\n",
      "     'training_loss': 0.3781682}\n",
      "train | step:   1665 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.0021478464,\n",
      "     'cls_loss': 0.26604265,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3734349,\n",
      "     'total_loss': 0.3734349,\n",
      "     'training_loss': 0.3734349}\n",
      "train | step:   1710 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0022108867,\n",
      "     'cls_loss': 0.2689062,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.37945056,\n",
      "     'total_loss': 0.37945056,\n",
      "     'training_loss': 0.37945056}\n",
      "train | step:   1755 | steps/sec:    1.5 | output: \n",
      "    {'box_loss': 0.002198734,\n",
      "     'cls_loss': 0.26622543,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.37616217,\n",
      "     'total_loss': 0.37616217,\n",
      "     'training_loss': 0.37616217}\n",
      "train | step:   1800 | steps/sec:    1.5 | output: \n",
      "    {'box_loss': 0.0021151032,\n",
      "     'cls_loss': 0.26110482,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.36685994,\n",
      "     'total_loss': 0.36685994,\n",
      "     'training_loss': 0.36685994}\n",
      " eval | step:   1800 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.64s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.04s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.674\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.514\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.506\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.563\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.638\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.647\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.146\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.692\n",
      " eval | step:   1800 | eval time:   59.8 sec | output: \n",
      "    {'AP': 0.47003204,\n",
      "     'AP50': 0.67361885,\n",
      "     'AP75': 0.5141746,\n",
      "     'APl': 0.5057134,\n",
      "     'APm': 0.031449683,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.691569,\n",
      "     'ARm': 0.14593986,\n",
      "     'ARmax1': 0.5632995,\n",
      "     'ARmax10': 0.6379285,\n",
      "     'ARmax100': 0.64693797,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0028204843,\n",
      "     'cls_loss': 0.3576336,\n",
      "     'model_loss': 0.4986578,\n",
      "     'total_loss': 0.4986578,\n",
      "     'validation_loss': 0.4986578}\n",
      "train | step:   1800 | training until step 2160...\n",
      "train | step:   1845 | steps/sec:    0.5 | output: \n",
      "    {'box_loss': 0.0021538176,\n",
      "     'cls_loss': 0.26374215,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.37143308,\n",
      "     'total_loss': 0.37143308,\n",
      "     'training_loss': 0.37143308}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/1637796032/ckpt-1845.\n",
      "train | step:   1890 | steps/sec:    1.9 | output: \n",
      "    {'box_loss': 0.0022076895,\n",
      "     'cls_loss': 0.26280192,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.37318638,\n",
      "     'total_loss': 0.37318638,\n",
      "     'training_loss': 0.37318638}\n",
      "train | step:   1935 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.0021441204,\n",
      "     'cls_loss': 0.25935933,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.36656532,\n",
      "     'total_loss': 0.36656532,\n",
      "     'training_loss': 0.36656532}\n",
      "train | step:   1980 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.002142912,\n",
      "     'cls_loss': 0.25962898,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3667746,\n",
      "     'total_loss': 0.3667746,\n",
      "     'training_loss': 0.3667746}\n",
      "train | step:   2025 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0021255855,\n",
      "     'cls_loss': 0.2578761,\n",
      "     'learning_rate': 0.004,\n",
      "     'model_loss': 0.3641553,\n",
      "     'total_loss': 0.3641553,\n",
      "     'training_loss': 0.3641553}\n",
      "train | step:   2070 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0021206033,\n",
      "     'cls_loss': 0.25572133,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.36175153,\n",
      "     'total_loss': 0.36175153,\n",
      "     'training_loss': 0.36175153}\n",
      "train | step:   2115 | steps/sec:    1.5 | output: \n",
      "    {'box_loss': 0.002061327,\n",
      "     'cls_loss': 0.253038,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35610437,\n",
      "     'total_loss': 0.35610437,\n",
      "     'training_loss': 0.35610437}\n",
      "train | step:   2160 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.00214785,\n",
      "     'cls_loss': 0.2560807,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.3634732,\n",
      "     'total_loss': 0.3634732,\n",
      "     'training_loss': 0.3634732}\n",
      " eval | step:   2160 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.19s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=3.91s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.684\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.527\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.516\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.567\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.639\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.647\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.146\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.691\n",
      " eval | step:   2160 | eval time:   59.8 sec | output: \n",
      "    {'AP': 0.47911617,\n",
      "     'AP50': 0.68407136,\n",
      "     'AP75': 0.5267093,\n",
      "     'APl': 0.5155282,\n",
      "     'APm': 0.032642324,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.6914793,\n",
      "     'ARm': 0.14634444,\n",
      "     'ARmax1': 0.5665515,\n",
      "     'ARmax10': 0.6385077,\n",
      "     'ARmax100': 0.6468874,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0027877188,\n",
      "     'cls_loss': 0.35512033,\n",
      "     'model_loss': 0.49450627,\n",
      "     'total_loss': 0.49450627,\n",
      "     'validation_loss': 0.49450627}\n",
      "train | step:   2160 | training until step 2520...\n",
      "train | step:   2205 | steps/sec:    0.5 | output: \n",
      "    {'box_loss': 0.0020659023,\n",
      "     'cls_loss': 0.25056806,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35386318,\n",
      "     'total_loss': 0.35386318,\n",
      "     'training_loss': 0.35386318}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/1637796032/ckpt-2205.\n",
      "train | step:   2250 | steps/sec:    2.0 | output: \n",
      "    {'box_loss': 0.0020829379,\n",
      "     'cls_loss': 0.2492491,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.353396,\n",
      "     'total_loss': 0.353396,\n",
      "     'training_loss': 0.353396}\n",
      "train | step:   2295 | steps/sec:    3.2 | output: \n",
      "    {'box_loss': 0.0020713964,\n",
      "     'cls_loss': 0.25005624,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.353626,\n",
      "     'total_loss': 0.353626,\n",
      "     'training_loss': 0.353626}\n",
      "train | step:   2340 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.002100987,\n",
      "     'cls_loss': 0.250392,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35544133,\n",
      "     'total_loss': 0.35544133,\n",
      "     'training_loss': 0.35544133}\n",
      "train | step:   2385 | steps/sec:    2.5 | output: \n",
      "    {'box_loss': 0.0020531043,\n",
      "     'cls_loss': 0.24699222,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.3496474,\n",
      "     'total_loss': 0.3496474,\n",
      "     'training_loss': 0.3496474}\n",
      "train | step:   2430 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0021170632,\n",
      "     'cls_loss': 0.25197142,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35782453,\n",
      "     'total_loss': 0.35782453,\n",
      "     'training_loss': 0.35782453}\n",
      "train | step:   2475 | steps/sec:    1.5 | output: \n",
      "    {'box_loss': 0.0020748933,\n",
      "     'cls_loss': 0.24476486,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.3485095,\n",
      "     'total_loss': 0.3485095,\n",
      "     'training_loss': 0.3485095}\n",
      "train | step:   2520 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0020252506,\n",
      "     'cls_loss': 0.24726908,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.34853163,\n",
      "     'total_loss': 0.34853163,\n",
      "     'training_loss': 0.34853163}\n",
      " eval | step:   2520 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.10s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=3.89s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.482\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.685\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.530\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.519\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.566\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.639\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.648\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.140\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.693\n",
      " eval | step:   2520 | eval time:   59.0 sec | output: \n",
      "    {'AP': 0.48206875,\n",
      "     'AP50': 0.6847014,\n",
      "     'AP75': 0.53009886,\n",
      "     'APl': 0.5186185,\n",
      "     'APm': 0.033925653,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.6933806,\n",
      "     'ARm': 0.13966972,\n",
      "     'ARmax1': 0.56610274,\n",
      "     'ARmax10': 0.6393364,\n",
      "     'ARmax100': 0.64811903,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0027638334,\n",
      "     'cls_loss': 0.3548692,\n",
      "     'model_loss': 0.4930609,\n",
      "     'total_loss': 0.4930609,\n",
      "     'validation_loss': 0.4930609}\n",
      "train | step:   2520 | training until step 2880...\n",
      "train | step:   2565 | steps/sec:    0.6 | output: \n",
      "    {'box_loss': 0.0020963422,\n",
      "     'cls_loss': 0.24770962,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.35252675,\n",
      "     'total_loss': 0.35252675,\n",
      "     'training_loss': 0.35252675}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/1637796032/ckpt-2565.\n",
      "train | step:   2610 | steps/sec:    1.8 | output: \n",
      "    {'box_loss': 0.0020450833,\n",
      "     'cls_loss': 0.24326983,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.34552407,\n",
      "     'total_loss': 0.34552407,\n",
      "     'training_loss': 0.34552407}\n",
      "train | step:   2655 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.002022641,\n",
      "     'cls_loss': 0.24196264,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.34309465,\n",
      "     'total_loss': 0.34309465,\n",
      "     'training_loss': 0.34309465}\n",
      "train | step:   2700 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.0020764945,\n",
      "     'cls_loss': 0.2425917,\n",
      "     'learning_rate': 0.002,\n",
      "     'model_loss': 0.34641644,\n",
      "     'total_loss': 0.34641644,\n",
      "     'training_loss': 0.34641644}\n",
      "train | step:   2745 | steps/sec:    1.6 | output: \n",
      "    {'box_loss': 0.0020294553,\n",
      "     'cls_loss': 0.2424696,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34394237,\n",
      "     'total_loss': 0.34394237,\n",
      "     'training_loss': 0.34394237}\n",
      "train | step:   2790 | steps/sec:    1.5 | output: \n",
      "    {'box_loss': 0.0020826072,\n",
      "     'cls_loss': 0.2452983,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34942862,\n",
      "     'total_loss': 0.34942862,\n",
      "     'training_loss': 0.34942862}\n",
      "train | step:   2835 | steps/sec:    1.5 | output: \n",
      "    {'box_loss': 0.0020406775,\n",
      "     'cls_loss': 0.2413942,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34342813,\n",
      "     'total_loss': 0.34342813,\n",
      "     'training_loss': 0.34342813}\n",
      "train | step:   2880 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0020209977,\n",
      "     'cls_loss': 0.2402018,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.3412516,\n",
      "     'total_loss': 0.3412516,\n",
      "     'training_loss': 0.3412516}\n",
      " eval | step:   2880 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=17.89s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=3.91s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.485\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.688\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.534\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.032\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.522\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.567\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.639\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.648\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.140\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.693\n",
      " eval | step:   2880 | eval time:   59.3 sec | output: \n",
      "    {'AP': 0.48485,\n",
      "     'AP50': 0.68774015,\n",
      "     'AP75': 0.53350365,\n",
      "     'APl': 0.5217125,\n",
      "     'APm': 0.032344975,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.6932551,\n",
      "     'ARm': 0.13977355,\n",
      "     'ARmax1': 0.56677145,\n",
      "     'ARmax10': 0.63918793,\n",
      "     'ARmax100': 0.6483656,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0027573076,\n",
      "     'cls_loss': 0.3555705,\n",
      "     'model_loss': 0.49343583,\n",
      "     'total_loss': 0.49343583,\n",
      "     'validation_loss': 0.49343583}\n",
      "train | step:   2880 | training until step 3240...\n",
      "train | step:   2925 | steps/sec:    0.6 | output: \n",
      "    {'box_loss': 0.0020332178,\n",
      "     'cls_loss': 0.23853707,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34019798,\n",
      "     'total_loss': 0.34019798,\n",
      "     'training_loss': 0.34019798}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/1637796032/ckpt-2925.\n",
      "train | step:   2970 | steps/sec:    2.0 | output: \n",
      "    {'box_loss': 0.0020433844,\n",
      "     'cls_loss': 0.24184312,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34401238,\n",
      "     'total_loss': 0.34401238,\n",
      "     'training_loss': 0.34401238}\n",
      "train | step:   3015 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.0020349852,\n",
      "     'cls_loss': 0.24044779,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34219706,\n",
      "     'total_loss': 0.34219706,\n",
      "     'training_loss': 0.34219706}\n",
      "train | step:   3060 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.0020173676,\n",
      "     'cls_loss': 0.23912682,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.33999518,\n",
      "     'total_loss': 0.33999518,\n",
      "     'training_loss': 0.33999518}\n",
      "train | step:   3105 | steps/sec:    1.5 | output: \n",
      "    {'box_loss': 0.0019899323,\n",
      "     'cls_loss': 0.23810047,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.33759716,\n",
      "     'total_loss': 0.33759716,\n",
      "     'training_loss': 0.33759716}\n",
      "train | step:   3150 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.002006567,\n",
      "     'cls_loss': 0.23869275,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.33902115,\n",
      "     'total_loss': 0.33902115,\n",
      "     'training_loss': 0.33902115}\n",
      "train | step:   3195 | steps/sec:    1.5 | output: \n",
      "    {'box_loss': 0.0020608136,\n",
      "     'cls_loss': 0.24079113,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.3438318,\n",
      "     'total_loss': 0.3438318,\n",
      "     'training_loss': 0.3438318}\n",
      "train | step:   3240 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0020424982,\n",
      "     'cls_loss': 0.24089222,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34301707,\n",
      "     'total_loss': 0.34301707,\n",
      "     'training_loss': 0.34301707}\n",
      " eval | step:   3240 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=15.20s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=3.82s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.486\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.689\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.533\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.523\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.567\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.639\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.649\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.693\n",
      " eval | step:   3240 | eval time:   56.2 sec | output: \n",
      "    {'AP': 0.4862201,\n",
      "     'AP50': 0.6887484,\n",
      "     'AP75': 0.53347725,\n",
      "     'APl': 0.5233238,\n",
      "     'APm': 0.030990466,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.6934929,\n",
      "     'ARm': 0.14341658,\n",
      "     'ARmax1': 0.5671299,\n",
      "     'ARmax10': 0.6393914,\n",
      "     'ARmax100': 0.6488783,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.0027521811,\n",
      "     'cls_loss': 0.353131,\n",
      "     'model_loss': 0.49074003,\n",
      "     'total_loss': 0.49074003,\n",
      "     'validation_loss': 0.49074003}\n",
      "train | step:   3240 | training until step 3600...\n",
      "train | step:   3285 | steps/sec:    0.5 | output: \n",
      "    {'box_loss': 0.0020613263,\n",
      "     'cls_loss': 0.23961131,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.34267753,\n",
      "     'total_loss': 0.34267753,\n",
      "     'training_loss': 0.34267753}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/1637796032/ckpt-3285.\n",
      "train | step:   3330 | steps/sec:    1.9 | output: \n",
      "    {'box_loss': 0.0019836922,\n",
      "     'cls_loss': 0.2362717,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.33545625,\n",
      "     'total_loss': 0.33545625,\n",
      "     'training_loss': 0.33545625}\n",
      "train | step:   3375 | steps/sec:    2.9 | output: \n",
      "    {'box_loss': 0.0019910694,\n",
      "     'cls_loss': 0.23702174,\n",
      "     'learning_rate': 0.001,\n",
      "     'model_loss': 0.33657518,\n",
      "     'total_loss': 0.33657518,\n",
      "     'training_loss': 0.33657518}\n",
      "train | step:   3420 | steps/sec:    3.3 | output: \n",
      "    {'box_loss': 0.0020088444,\n",
      "     'cls_loss': 0.23592363,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.33636585,\n",
      "     'total_loss': 0.33636585,\n",
      "     'training_loss': 0.33636585}\n",
      "train | step:   3465 | steps/sec:    2.3 | output: \n",
      "    {'box_loss': 0.0019883802,\n",
      "     'cls_loss': 0.23618281,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.33560184,\n",
      "     'total_loss': 0.33560184,\n",
      "     'training_loss': 0.33560184}\n",
      "train | step:   3510 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0019986914,\n",
      "     'cls_loss': 0.23567718,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.3356118,\n",
      "     'total_loss': 0.3356118,\n",
      "     'training_loss': 0.3356118}\n",
      "train | step:   3555 | steps/sec:    1.5 | output: \n",
      "    {'box_loss': 0.0020124393,\n",
      "     'cls_loss': 0.23597863,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.33660063,\n",
      "     'total_loss': 0.33660063,\n",
      "     'training_loss': 0.33660063}\n",
      "train | step:   3600 | steps/sec:    1.4 | output: \n",
      "    {'box_loss': 0.0019992497,\n",
      "     'cls_loss': 0.23560098,\n",
      "     'learning_rate': 0.0005,\n",
      "     'model_loss': 0.33556348,\n",
      "     'total_loss': 0.33556348,\n",
      "     'training_loss': 0.33556348}\n",
      " eval | step:   3600 | running 14 steps of evaluation...\n",
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=18.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=3.85s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.488\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.690\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.535\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.526\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.568\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.641\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.137\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.695\n",
      " eval | step:   3600 | eval time:   60.6 sec | output: \n",
      "    {'AP': 0.48846987,\n",
      "     'AP50': 0.6900665,\n",
      "     'AP75': 0.5348784,\n",
      "     'APl': 0.52582955,\n",
      "     'APm': 0.032519117,\n",
      "     'APs': 0.0,\n",
      "     'ARl': 0.694866,\n",
      "     'ARm': 0.1367141,\n",
      "     'ARmax1': 0.56847763,\n",
      "     'ARmax10': 0.6409898,\n",
      "     'ARmax100': 0.64970815,\n",
      "     'ARs': 0.0,\n",
      "     'box_loss': 0.002743904,\n",
      "     'cls_loss': 0.35398468,\n",
      "     'model_loss': 0.49117988,\n",
      "     'total_loss': 0.49117988,\n",
      "     'validation_loss': 0.49117988}\n",
      "saved checkpoint to gs://ml1-demo-martin/arthropod_jobs/1637796032/ckpt-3600.\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_DIR)\n",
    "model,_ = train_lib.run_experiment(\n",
    "    distribution_strategy=strategy,\n",
    "    task=task,\n",
    "    mode=\"train_and_eval\", # 'train', 'eval', 'train_and_eval' or 'continuous_eval'\n",
    "    params=params,\n",
    "    model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the model\n",
    "To test the exported model, please use the notebook \"04ac_retinanet_arthropods_predict.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.beta.modeling.retinanet_model.RetinaNetModel object at 0x7f086ff9a5d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.beta.modeling.retinanet_model.RetinaNetModel object at 0x7f086ff9a5d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.beta.modeling.layers.detection_generator.MultilevelDetectionGenerator object at 0x7f086c08ff50>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <official.vision.beta.modeling.layers.detection_generator.MultilevelDetectionGenerator object at 0x7f086c08ff50>, because it is not built.\n",
      "2021-11-25 00:07:09.126305: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as inference_from_image_bytes, inference_from_tf_example, retina_net_head_1_layer_call_fn, retina_net_head_1_layer_call_and_return_conditional_losses, scores_layer_call_fn while saving (showing 5 of 597). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://ml1-demo-martin/arthropod_jobs/1637796032/saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: gs://ml1-demo-martin/arthropod_jobs/1637796032/saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "export_saved_model_lib.export_inference_graph(\n",
    "      input_type='image_tensor',\n",
    "      batch_size=4,\n",
    "      input_image_size=IMAGE_SIZE,\n",
    "      params=params,\n",
    "      checkpoint_path=MODEL_DIR,\n",
    "      export_dir=MODEL_DIR,\n",
    "      export_checkpoint_subdir='saved_chkpt',\n",
    "      export_saved_model_subdir='saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n",
    "Copyright 2021 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-7.m86",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-7:m86"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
